{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo 02: Previsão de feedbacks de produtos B2W**\n",
    "\n",
    "Você recebeu um convite para uma consultoria, na qual deve desenvolver um modelo de previsões de feedbacks de clientes em produtos comprados na loja, que serão coletados do instagram.\n",
    "\n",
    "Os dados que você vai utilizar estão localizados em:\n",
    "https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv\n",
    "\n",
    "Na coluna 'review_title' você vai encontrar feedbacks passados dos nossos clientes em nossos produtos e, na coluna 'overall_rating', a nota que foi dada. Esse é o único dado que temos para auxiliar na criação desse modelo de previsões.\n",
    "\n",
    "Dúvidas? Fale comigo!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importando bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtendo dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtendo dados...\n",
      "Erro ao obter dados:  '[False] not in index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Local\\Temp\\ipykernel_27432\\1081234357.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')[['review_state'=='RJ','overall_rating']]\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "try:\n",
    "    print('Obtendo dados...')\n",
    "\n",
    "    # constante dos dados\n",
    "    ENDERECO_DADOS = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv'\n",
    "\n",
    "    # obtendo dados\n",
    "    df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')[['review_state'=='RJ','overall_rating']]\n",
    "\n",
    "    # excluindo dados não existentes (NaN)\n",
    "    df = df.dropna(subset=['review_title' ,'overall_rating'])\n",
    "\n",
    "    # Tranformando colunas em arrays\n",
    "    texts = np.array(df['review_title'])\n",
    "    rating = np.array(df['overall_rating'])\n",
    "\n",
    "    # exibir o total de valores por categoria\n",
    "    print(df['overall_rating'].value_counts())\n",
    "\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print('Erro ao obter dados: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vetorização**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetorizando texto...\n",
      "[[   0    0    0 ...    0    0    3]\n",
      " [   0    0    0 ... 2620   30   16]\n",
      " [   0    0    0 ...  349   45  155]\n",
      " ...\n",
      " [   0    0    0 ...    0    9    1]\n",
      " [   0    0    0 ...    4   19    3]\n",
      " [   0    0    0 ...    1    4   51]]\n",
      "Textos vetorizados!\n"
     ]
    }
   ],
   "source": [
    "# Biblioteca para trabalhar com redes neurais artificiais\n",
    "# Tensorflow - https://www.tensorflow.org/?hl=pt-br\n",
    "# Tokenizar\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "# ajustar o tamanho do vetor\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "\n",
    "try:\n",
    "    print('Vetorizando texto...')\n",
    "\n",
    "    # Passo 1: tokenizar\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Passo 2: Criar o dicionário\n",
    "    # fit_on_texts: Cria o vocabulário, através do dicionário\n",
    "    # associando cada token a um índice numérico\n",
    "    # lembrando que se a palavra aparecer mais de uma vez, ela vai receber o mesmo índice numérico\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Passo 3: Vetorizar, ou seja, transformar os tokens em números,\n",
    "    # a partir do dicionário criado no passo 2\n",
    "    vetores = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Passo4: Padronização do tamanho do vetor - pad\n",
    "    padded_vetores = pad_sequences(vetores)\n",
    "\n",
    "    print(padded_vetores)\n",
    "\n",
    "    print('Textos vetorizados!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Erro ao vetorizar textos: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rede Neural**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construindo a rede neural...\n",
      "Modelo configurado e criado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definição do modelo de rede neural utilizada\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "# Camadas da rede neural\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout # type: ignore\n",
    "# Otimizador de taxa de aprendizado\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "try:\n",
    "    print('Construindo a rede neural...')\n",
    "\n",
    "    # Constantes do modelo\n",
    "\n",
    "    # 1ª Constante: Tamanho do vocabulário\n",
    "    VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # 2ª Constante: Tamanho máximo da sequência\n",
    "    # É o comprimento máximo de um texto\n",
    "    MAX_SEQUENCE_LENGHT = padded_vetores.shape[1]\n",
    "\n",
    "    # 3ª Constante: Tamanho do vetor de entrada\n",
    "    # A literatura recomenda que inicia-se por uma quantidade \n",
    "    # igual a raiz quadrada do tamanho do vocabulário\n",
    "    # Se o volume de dados for de larga escala, pode-se testar iniciando em um tamanho maior\n",
    "    # Se o volume de dados for muito pequeno, pode-se testar inciando com um tamanho menor\n",
    "    # Cuidado com o overfitting, que é quando o modelo \"aprende demais\" e, começa a \n",
    "    # perder a capacidade de generalizar melhor, ou seja, obsevar todas as diferenças textuais\n",
    "    # Overfitting pode ser observado no treino da rede neural\n",
    "    VETOR_LENGHT = int(np.sqrt(VOCAB_SIZE))\n",
    "\n",
    "    # Inicia-se a construção da rede neural\n",
    "    # Sequential é um fluxo linear de camandas (conforme visto na Aula02_RNA.pptx)\n",
    "    # São processadas em ordem\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    # Embeddings, na qual os vetores de texto são inseridos\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=VOCAB_SIZE,\n",
    "            output_dim=VETOR_LENGHT,\n",
    "            input_length=MAX_SEQUENCE_LENGHT))\n",
    "\n",
    "    # Camada oculta ou intermediária\n",
    "    # LSTM - Long short-term memory, em português \"memória de curto e longo prazo\"\n",
    "    # É onde a magia acontece, ou seja, onde o modelo treina baseado nos seus vetores\n",
    "    # Números de unidades de memória, que basicamente é a qtde de \"neurônios\"\n",
    "    # Primeiro TESTE, EXPERIMENTE somente com 1 camada! Cuidado! com o ovberfitting!\n",
    "    # Se for necessário, adicionar mais camadas, basta repetir o comando abaixo\n",
    "    \n",
    "    # primeirca camada oculta\n",
    "    model.add(LSTM(128))\n",
    "\n",
    "    # Se necessário, adicionar outra camada oculta\n",
    "    # model.add(LSTM(64))\n",
    "\n",
    "    # Camada de saída - Camada Densa\n",
    "    # Classificação, que é o caso desse exemplo.\n",
    "    # Precisa ajustar para a quantidade de classes/categorias\n",
    "    # 1, 2, 3, 4, 5\n",
    "    # Em RNA de Classidificação (classes/categorias)\n",
    "    # A função de ativação mais utilizada é a softmax\n",
    "    # Função de ativação é um cálculo matemático \n",
    "    # que vai determinar a saída de cada neurônio\n",
    "    # Softmax é uma função (mais utilizada em classificação) \n",
    "    # que transforma os valores de saída\n",
    "    # em PROBABILIDADES que vão de 0 a 1\n",
    "    # A soma das classes, no nosso caso 5 classes, será igual 1\n",
    "    # A outra função mais utilizada ReLu (que tb pode ser utilizada em classificação)\n",
    "    # É uma função que retorna 0, para saídas negativas e o valor original, para\n",
    "    # saidas maiores que 0 (O que faz ela ser mais indicada para Regressão)\n",
    "    # model.add(Dense(1, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Dropout é um técnica de regularização do resultado, pra minimizar o overfitting\n",
    "    # Ele desabilita neurônios aleatoriamente, justamente, para tentar minimizar overfitting\n",
    "    # Cuidado com o underfitting, se reduzir demais, vai dar ruim!\n",
    "    model.add(Dropout(0.05)) # desativar 5% dos neurônios, aleatoriamente\n",
    "\n",
    "    # construir o modelo\n",
    "    # É literamente pegar as definições anteriores e construir o modelo\n",
    "    # input_shape: é o formato do dados de entrada e ainda o tamanho máximo do texto (MAX_SEQUENCE_LENGHT)\n",
    "    model.build(input_shape=(None,MAX_SEQUENCE_LENGHT))\n",
    "    \n",
    "    # Otimizador de taxa de aprendizado\n",
    "    # importante para ajustar, em casos de overfitting\n",
    "    # Adam: É nosso otimizador que vaia ajustar essa taxa de aprendizado\n",
    "    # learning_rate: Quanto menor, melhor o aprendizado. Menos risco de overfitting\n",
    "    otimizador = Adam(learning_rate=0.0001)\n",
    "\n",
    "    # compilar o modelo\n",
    "    # Verificar se possui algum erro ou se tá de boa\n",
    "    # Além disso, vamos informar o otimizador e a nossa métrica de perda (loss)\n",
    "    model.compile(optimizer=otimizador, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    #model.summary()\n",
    "    print('Modelo configurado e criado')\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao construir a rede neural: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinar o modelo de rede neural\n",
      "overall_rating\n",
      "5     2.759758\n",
      "4     4.090785\n",
      "1     4.841312\n",
      "3     8.112968\n",
      "2    15.777207\n",
      "Name: count, dtype: float64\n",
      "Epoch 1/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 17ms/step - loss: 11.3596 - val_loss: 1.1988\n",
      "Epoch 2/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 18ms/step - loss: 9.9046 - val_loss: 1.2185\n",
      "Epoch 3/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 18ms/step - loss: 9.6073 - val_loss: 1.1101\n",
      "Epoch 4/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 18ms/step - loss: 9.8057 - val_loss: 1.1291\n",
      "Epoch 5/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 18ms/step - loss: 9.5248 - val_loss: 1.1002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split#type:ignore\n",
    "\n",
    "try:\n",
    "    print('Treinar o modelo de rede neural')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_vetores,\n",
    "        rating,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Na Classificação\n",
    "    # Qdo a gente constroi um RNA de classificação\n",
    "    # os rótulo que a RNA atribui [0,1,2,3,4] e os meus dados são [1,2,3,4,5]\n",
    "    y_train_adjusted = y_train - 1\n",
    "    y_test_adjusted = y_test - 1\n",
    "\n",
    "    # Aplicar pesos as categorias\n",
    "    pesos = len(df['overall_rating'])/df['overall_rating'].value_counts()\n",
    "\n",
    "    print(pesos)\n",
    "\n",
    "    # o treino da rede neural\n",
    "    model.fit(\n",
    "        X_train, # x treino\n",
    "        y_train_adjusted, # y treino\n",
    "        epochs=5, # Épocas de estudo da rede neural. Qto mais epochs, mais aprendizado, \n",
    "        # mais processamento, porém cuidado com o overfitting. A regre aqui é experimentação\n",
    "        # avaliando a peda de treino, com a perda de teste\n",
    "        batch_size=32, # Qto maior tamanho da batch, tende a um menor o aprendizado, porém evita overfitting\n",
    "        # Qto menor o tamanho da batch, tende a um maior aprendizado\n",
    "        class_weight = pesos.to_dict(), # precisa receber os pesos em uma estrutura de dados, do tipo dicionário\n",
    "        validation_data=(X_test, y_test_adjusted)\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao treinar a rede neural: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Previsões:  [[0.0001 0.0002 0.0056 0.2863 0.7078]\n",
      " [0.8209 0.0881 0.0372 0.0237 0.03  ]]\n"
     ]
    }
   ],
   "source": [
    "# textos para realizar a previsão, baseada no modelo de rede neural\n",
    "# Pode e deve conectar em um fonte de dados, para realizar as previsões de sentimentos\n",
    "novos_textos = [\n",
    "    \"Muito bom, gostei bastante. Top demais! Compensa muito!\",\n",
    "    \"Não recomendo, péssimo produto. Não funciona\"\n",
    "]\n",
    "\n",
    "# Vetorizar\n",
    "novas_sequencias = tokenizer.texts_to_sequences(novos_textos)\n",
    "novas_sequencias_padded = pad_sequences(novas_sequencias)\n",
    "\n",
    "# análise preditiva\n",
    "predicoes = model.predict(novas_sequencias_padded)\n",
    "\n",
    "# Formatar valores de saída\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "print(\"Previsões: \", predicoes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
